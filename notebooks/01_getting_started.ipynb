{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5226617a",
   "metadata": {},
   "source": [
    "# üöÄ ML Project Framework - Getting Started\n",
    "\n",
    "A comprehensive guide to using the production-ready ML Project Framework for end-to-end machine learning workflows.\n",
    "\n",
    "This notebook demonstrates:\n",
    "\n",
    "- Environment setup and dependency management\n",
    "- Configuration management with YAML\n",
    "- Complete data pipeline (load, clean, split)\n",
    "- Feature engineering and preprocessing\n",
    "- Model training and hyperparameter tuning\n",
    "- Comprehensive model evaluation\n",
    "- Experiment tracking and result management\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418e9898",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Environment Setup and Virtual Environment Creation\n",
    "\n",
    "First, let's set up the development environment with project isolation using a virtual environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396769f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Python version and environment info\n",
    "import sys\n",
    "import os\n",
    "\n",
    "print(f\"Python Version: {sys.version}\")\n",
    "print(f\"Python Executable: {sys.executable}\")\n",
    "print(f\"Current Working Directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9fa063",
   "metadata": {},
   "source": [
    "### Virtual Environment Setup (Optional - do this in terminal)\n",
    "\n",
    "```bash\n",
    "# Create virtual environment\n",
    "python -m venv venv\n",
    "\n",
    "# Activate virtual environment\n",
    "# On Windows:\n",
    "venv\\Scripts\\activate\n",
    "\n",
    "# On macOS/Linux:\n",
    "source venv/bin/activate\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a12624",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Install Dependencies and Package Installation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbb1eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "import subprocess\n",
    "\n",
    "# Check if packages are already installed\n",
    "try:\n",
    "    import pandas\n",
    "    import numpy\n",
    "    import sklearn\n",
    "    print(\"‚úì Required packages already installed\")\n",
    "except ImportError:\n",
    "    print(\"Installing required packages...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-r\", \"requirements.txt\"])\n",
    "    print(\"‚úì Packages installed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504c866a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the project in editable mode\n",
    "try:\n",
    "    import src\n",
    "    print(\"‚úì ML Project Framework is already installed\")\n",
    "except ImportError:\n",
    "    print(\"Installing ML Project Framework...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-e\", \".\"])\n",
    "    print(\"‚úì Framework installed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d035c5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_classification, make_regression\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"‚úì All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf439d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import framework modules\n",
    "from src.utils import get_logger, load_config\n",
    "from src.data import DataProcessor\n",
    "from src.features import FeatureEngineer, build_features\n",
    "from src.models import ModelTrainer\n",
    "from src.evaluation import evaluate_model, plot_confusion_matrix, plot_feature_importance\n",
    "\n",
    "print(\"‚úì Framework modules imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14af4ed5",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Load and Explore Configuration Management\n",
    "\n",
    "Load and display the YAML configuration file that controls the ML pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cc5ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize logger\n",
    "logger = get_logger('ml_pipeline', log_dir='logs')\n",
    "logger.info(\"Starting ML Pipeline Tutorial\")\n",
    "\n",
    "# Load configuration\n",
    "config = load_config()\n",
    "config_dict = config.get_all()\n",
    "\n",
    "print(\"‚úì Configuration loaded successfully\\n\")\n",
    "print(\"Configuration Overview:\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447076a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display configuration sections\n",
    "print(\"Project Configuration:\")\n",
    "print(f\"  Name: {config.get('project.name')}\")\n",
    "print(f\"  Description: {config.get('project.description')}\")\n",
    "print(f\"  Version: {config.get('project.version')}\")\n",
    "\n",
    "print(\"\\nProblem Configuration:\")\n",
    "print(f\"  Type: {config.get('problem.type')}\")\n",
    "print(f\"  Task: {config.get('problem.task')}\")\n",
    "\n",
    "print(\"\\nData Configuration:\")\n",
    "print(f\"  Target Variable: {config.get('data.target_variable')}\")\n",
    "print(f\"  Test Split: {config.get('data.test_split')}\")\n",
    "print(f\"  Validation Split: {config.get('data.validation_split')}\")\n",
    "\n",
    "print(\"\\nModel Configuration:\")\n",
    "print(f\"  Algorithm: {config.get('model.algorithm')}\")\n",
    "print(f\"  Hyperparameters: {config.get('model.params')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a3777a",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Data Pipeline - Loading and Cleaning\n",
    "\n",
    "Implement the complete data loading and cleaning pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a822a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample dataset for demonstration\n",
    "print(\"üìä Creating sample dataset for demonstration...\")\n",
    "\n",
    "problem_type = config.get('problem.type', 'classification')\n",
    "\n",
    "if problem_type == 'classification':\n",
    "    X, y = make_classification(\n",
    "        n_samples=1000,\n",
    "        n_features=15,\n",
    "        n_informative=10,\n",
    "        n_redundant=3,\n",
    "        n_classes=2,\n",
    "        random_state=42\n",
    "    )\n",
    "else:\n",
    "    X, y = make_regression(\n",
    "        n_samples=1000,\n",
    "        n_features=15,\n",
    "        n_informative=10,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "df = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(X.shape[1])])\n",
    "df[config.get('data.target_variable', 'target')] = y\n",
    "\n",
    "print(f\"‚úì Dataset created: {df.shape}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f952b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore dataset characteristics\n",
    "print(\"Dataset Statistics:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "processor = DataProcessor(random_state=42)\n",
    "summary = processor.get_data_summary(df)\n",
    "\n",
    "print(f\"Shape: {summary['shape']}\")\n",
    "print(f\"\\nData Types:\")\n",
    "for col, dtype in summary['dtypes'].items():\n",
    "    print(f\"  {col}: {dtype}\")\n",
    "\n",
    "print(f\"\\nMissing Values:\")\n",
    "if all(v == 0 for v in summary['missing_values'].values()):\n",
    "    print(\"  No missing values found ‚úì\")\n",
    "else:\n",
    "    for col, count in summary['missing_values'].items():\n",
    "        if count > 0:\n",
    "            print(f\"  {col}: {count} ({summary['missing_percentage'][col]:.2f}%)\")\n",
    "\n",
    "print(f\"\\nDuplicate Rows: {summary['duplicates']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658150b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean data\n",
    "print(\"Data Cleaning Pipeline:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Handle missing values\n",
    "df_clean = processor.handle_missing_values(df, strategy='drop')\n",
    "print(f\"‚úì Missing values handled\")\n",
    "\n",
    "# Remove duplicates\n",
    "df_clean = processor.remove_duplicates(df_clean)\n",
    "print(f\"‚úì Duplicates removed\")\n",
    "\n",
    "# Display cleaned data info\n",
    "summary_clean = processor.get_data_summary(df_clean)\n",
    "print(f\"\\nCleaned Dataset Shape: {summary_clean['shape']}\")\n",
    "print(f\"Rows removed: {df.shape[0] - df_clean.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3729ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data splitting\n",
    "print(\"\\nData Splitting:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "target_col = config.get('data.target_variable', 'target')\n",
    "test_size = config.get('data.test_split', 0.2)\n",
    "\n",
    "X_train, X_test, y_train, y_test = processor.split_data(\n",
    "    df_clean,\n",
    "    target_col=target_col,\n",
    "    test_size=test_size\n",
    ")\n",
    "\n",
    "print(f\"Training Set: {X_train.shape[0]} samples ({(1-test_size)*100:.0f}%)\")\n",
    "print(f\"Test Set: {X_test.shape[0]} samples ({test_size*100:.0f}%)\")\n",
    "print(f\"Features: {X_train.shape[1]}\")\n",
    "print(f\"\\nTarget Distribution (Training):\")\n",
    "print(y_train.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531e1dea",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Feature Engineering and Preprocessing\n",
    "\n",
    "Apply feature scaling, encoding, and engineering to create more informative features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f13fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Feature Engineering Pipeline:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "engineer = FeatureEngineer()\n",
    "\n",
    "# Scale features\n",
    "scaling_method = config.get('features.scaling.method', 'standard')\n",
    "X_train_scaled = engineer.scale_features(X_train, method=scaling_method, fit=True)\n",
    "X_test_scaled = engineer.scale_features(X_test, method=scaling_method, fit=False)\n",
    "\n",
    "print(f\"‚úì Features scaled using {scaling_method} scaling\")\n",
    "print(f\"  Train shape: {X_train_scaled.shape}\")\n",
    "print(f\"  Test shape: {X_test_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba59c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display feature statistics before and after scaling\n",
    "print(\"\\nFeature Statistics (Before Scaling):\")\n",
    "print(X_train.iloc[:5].describe().loc[['mean', 'std', 'min', 'max']])\n",
    "\n",
    "print(\"\\nFeature Statistics (After Scaling):\")\n",
    "print(X_train_scaled.iloc[:5].describe().loc[['mean', 'std', 'min', 'max']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730c6e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create polynomial features (optional)\n",
    "use_poly = config.get('features.engineering.polynomial_features.enabled', False)\n",
    "if use_poly:\n",
    "    poly_degree = config.get('features.engineering.polynomial_features.degree', 2)\n",
    "    X_train_eng = engineer.create_polynomial_features(X_train_scaled, degree=poly_degree, fit=True)\n",
    "    X_test_eng = engineer.create_polynomial_features(X_test_scaled, degree=poly_degree, fit=False)\n",
    "    print(f\"‚úì Polynomial features created (degree={poly_degree})\")\n",
    "    print(f\"  Train shape: {X_train_eng.shape}\")\n",
    "    print(f\"  Test shape: {X_test_eng.shape}\")\n",
    "else:\n",
    "    X_train_eng = X_train_scaled.copy()\n",
    "    X_test_eng = X_test_scaled.copy()\n",
    "    print(\"‚úì Polynomial features disabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7d05f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nFeature Engineering Summary:\")\n",
    "print(f\"  Original Features: {X_train.shape[1]}\")\n",
    "print(f\"  Engineered Features: {X_train_eng.shape[1]}\")\n",
    "print(f\"  Feature Increase: {X_train_eng.shape[1] - X_train.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91b8b8a",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Model Training with Multiple Algorithms\n",
    "\n",
    "Train machine learning models using different algorithms and hyperparameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507cea88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model trainer\n",
    "trainer = ModelTrainer(random_state=42)\n",
    "\n",
    "# Get configuration\n",
    "algorithm = config.get('model.algorithm', 'random_forest')\n",
    "model_params = config.get('model.params', {})\n",
    "problem_type = config.get('problem.type', 'classification')\n",
    "\n",
    "print(\"Model Training:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Algorithm: {algorithm}\")\n",
    "print(f\"Problem Type: {problem_type}\")\n",
    "print(f\"Hyperparameters: {model_params}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0072c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the primary model\n",
    "trainer.train(\n",
    "    X_train_eng,\n",
    "    y_train,\n",
    "    algorithm=algorithm,\n",
    "    problem_type=problem_type,\n",
    "    params=model_params\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úì {algorithm} model trained successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f425763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display model information\n",
    "print(f\"\\nModel Details:\")\n",
    "print(f\"  Type: {type(trainer.model).__name__}\")\n",
    "print(f\"  Training Samples: {trainer.training_history['train_samples']}\")\n",
    "print(f\"  Features: {trainer.training_history['train_features']}\")\n",
    "print(f\"  Trained At: {trainer.training_history['timestamp']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481dba5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train alternative models for comparison\n",
    "print(\"\\nTraining Alternative Models for Comparison:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "alternative_algorithms = ['gradient_boosting', 'logistic_regression'] if problem_type == 'classification' else ['gradient_boosting']\n",
    "alternative_models = {}\n",
    "\n",
    "for alt_algo in alternative_algorithms:\n",
    "    try:\n",
    "        alt_trainer = ModelTrainer(random_state=42)\n",
    "        alt_trainer.train(\n",
    "            X_train_eng,\n",
    "            y_train,\n",
    "            algorithm=alt_algo,\n",
    "            problem_type=problem_type,\n",
    "            params={}\n",
    "        )\n",
    "        alternative_models[alt_algo] = alt_trainer\n",
    "        print(f\"‚úì {alt_algo} model trained\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚úó {alt_algo} model failed: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27153bd2",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Model Evaluation and Metrics Visualization\n",
    "\n",
    "Evaluate model performance using comprehensive metrics and visualizations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a62475e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "print(\"Model Evaluation:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "y_train_pred = trainer.predict(X_train_eng)\n",
    "y_test_pred = trainer.predict(X_test_eng)\n",
    "\n",
    "print(f\"‚úì Predictions generated\")\n",
    "print(f\"  Train predictions shape: {y_train_pred.shape}\")\n",
    "print(f\"  Test predictions shape: {y_test_pred.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55082589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get prediction probabilities for classification\n",
    "if problem_type == 'classification' and hasattr(trainer.model, 'predict_proba'):\n",
    "    y_train_proba = trainer.predict_proba(X_train_eng)\n",
    "    y_test_proba = trainer.predict_proba(X_test_eng)\n",
    "    print(f\"‚úì Prediction probabilities generated\")\n",
    "    print(f\"  Train probabilities shape: {y_train_proba.shape}\")\n",
    "    print(f\"  Test probabilities shape: {y_test_proba.shape}\")\n",
    "else:\n",
    "    y_train_proba = None\n",
    "    y_test_proba = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52474987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on training set\n",
    "print(\"\\nTraining Set Performance:\")\n",
    "train_metrics = evaluate_model(\n",
    "    y_train,\n",
    "    y_train_pred,\n",
    "    problem_type=problem_type,\n",
    "    y_proba=y_train_proba\n",
    ")\n",
    "\n",
    "for key, value in train_metrics.items():\n",
    "    if key not in ['confusion_matrix', 'classification_report']:\n",
    "        if isinstance(value, float):\n",
    "            print(f\"  {key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401dd5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "print(\"\\nTest Set Performance:\")\n",
    "test_metrics = evaluate_model(\n",
    "    y_test,\n",
    "    y_test_pred,\n",
    "    problem_type=problem_type,\n",
    "    y_proba=y_test_proba\n",
    ")\n",
    "\n",
    "for key, value in test_metrics.items():\n",
    "    if key not in ['confusion_matrix', 'classification_report']:\n",
    "        if isinstance(value, float):\n",
    "            print(f\"  {key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f81eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize confusion matrix\n",
    "if problem_type == 'classification':\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    cm_data = plot_confusion_matrix(y_test, y_test_pred)\n",
    "    cm = np.array(cm_data['matrix'])\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "    plt.title('Confusion Matrix - Test Set')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\n{cm_data['title']}\")\n",
    "    print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c5ab52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "if hasattr(trainer.model, 'feature_importances_'):\n",
    "    print(\"\\nFeature Importance Analysis:\")\n",
    "    feature_names = X_train_eng.columns\n",
    "    importances = trainer.model.feature_importances_\n",
    "    feature_importance_dict = dict(zip(feature_names, importances))\n",
    "    \n",
    "    # Plot top 10 features\n",
    "    top_n = min(10, len(feature_importance_dict))\n",
    "    importance_data = plot_feature_importance(feature_importance_dict, top_n=top_n)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(importance_data['features'], importance_data['importance'])\n",
    "    plt.xlabel('Importance')\n",
    "    plt.title(importance_data['title'])\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nTop {top_n} Features:\")\n",
    "    for feat, imp in zip(importance_data['features'], importance_data['importance']):\n",
    "        print(f\"  {feat}: {imp:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971115da",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Experiment Tracking and Results Management\n",
    "\n",
    "Save and manage experiment results, models, and metrics systematically.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d35a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "print(\"Experiment Management:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "model_path = f'models/final/{algorithm}_model_{timestamp}.pkl'\n",
    "\n",
    "try:\n",
    "    os.makedirs(os.path.dirname(model_path), exist_ok=True)\n",
    "    trainer.save_model(model_path)\n",
    "    print(f\"‚úì Model saved: {model_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚úó Failed to save model: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3a1662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save metrics\n",
    "metrics_path = f'experiments/metrics_{timestamp}.json'\n",
    "\n",
    "try:\n",
    "    os.makedirs(os.path.dirname(metrics_path), exist_ok=True)\n",
    "    \n",
    "    # Prepare metrics for JSON serialization\n",
    "    metrics_to_save = {}\n",
    "    for key, value in test_metrics.items():\n",
    "        if key not in ['confusion_matrix', 'classification_report']:\n",
    "            if isinstance(value, (int, float)):\n",
    "                metrics_to_save[key] = value\n",
    "            elif hasattr(value, 'tolist'):\n",
    "                metrics_to_save[key] = value.tolist()\n",
    "            else:\n",
    "                metrics_to_save[key] = str(value)\n",
    "    \n",
    "    with open(metrics_path, 'w') as f:\n",
    "        json.dump(metrics_to_save, f, indent=2)\n",
    "    \n",
    "    print(f\"‚úì Metrics saved: {metrics_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚úó Failed to save metrics: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464fdde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save experiment summary\n",
    "summary_path = f'experiments/experiment_summary_{timestamp}.json'\n",
    "\n",
    "try:\n",
    "    experiment_summary = {\n",
    "        'timestamp': timestamp,\n",
    "        'algorithm': algorithm,\n",
    "        'problem_type': problem_type,\n",
    "        'configuration': {\n",
    "            'hyperparameters': model_params,\n",
    "            'train_samples': X_train_eng.shape[0],\n",
    "            'test_samples': X_test_eng.shape[0],\n",
    "            'features': X_train_eng.shape[1]\n",
    "        },\n",
    "        'performance': {key: value for key, value in test_metrics.items() if isinstance(value, (int, float))},\n",
    "        'model_path': model_path,\n",
    "        'metrics_path': metrics_path\n",
    "    }\n",
    "    \n",
    "    with open(summary_path, 'w') as f:\n",
    "        json.dump(experiment_summary, f, indent=2)\n",
    "    \n",
    "    print(f\"‚úì Experiment summary saved: {summary_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚úó Failed to save summary: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c26507",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ Running the End-to-End Pipeline\n",
    "\n",
    "Execute the complete machine learning pipeline with a single function call.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b3208e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nComplete Pipeline Execution Summary:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\n‚úì Steps Completed:\")\n",
    "print(f\"  1. Environment setup and dependency installation\")\n",
    "print(f\"  2. Configuration loaded from config.yaml\")\n",
    "print(f\"  3. Data loaded and cleaned: {df_clean.shape}\")\n",
    "print(f\"  4. Train/test split: {X_train_eng.shape[0]}/{X_test_eng.shape[0]}\")\n",
    "print(f\"  5. Features engineered: {X_train.shape[1]} ‚Üí {X_train_eng.shape[1]}\")\n",
    "print(f\"  6. Model trained: {algorithm}\")\n",
    "print(f\"  7. Performance evaluated\")\n",
    "print(f\"  8. Results saved to experiments/\")\n",
    "print(f\"  9. Model saved to models/final/\")\n",
    "\n",
    "print(f\"\\nüìä Final Performance:\")\n",
    "for key, value in test_metrics.items():\n",
    "    if key not in ['confusion_matrix', 'classification_report']:\n",
    "        if isinstance(value, float):\n",
    "            print(f\"  {key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b2a6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìÅ Output Artifacts:\")\n",
    "print(f\"  Model: {model_path}\")\n",
    "print(f\"  Metrics: {metrics_path}\")\n",
    "print(f\"  Summary: {summary_path}\")\n",
    "print(f\"  Logs: logs/\")\n",
    "\n",
    "print(\"\\nüéâ Pipeline execution complete!\")\n",
    "print(\"\\nüí° Next Steps:\")\n",
    "print(\"  1. Modify config.yaml to customize hyperparameters\")\n",
    "print(\"  2. Add your own data to data/raw/\")\n",
    "print(\"  3. Experiment with different algorithms\")\n",
    "print(\"  4. Run: python run_pipeline.py for batch execution\")\n",
    "print(\"  5. Review results in experiments/ directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdd46aa",
   "metadata": {},
   "source": [
    "## üìö Next Steps\n",
    "\n",
    "### Customize Your Project\n",
    "\n",
    "1. **Edit Configuration**: Modify `config/config.yaml` to change algorithms, hyperparameters, and data sources\n",
    "2. **Add Your Data**: Place your dataset in `data/raw/` and update the configuration\n",
    "3. **Run Pipeline**: Execute `python run_pipeline.py` for batch processing\n",
    "4. **Analyze Results**: Check `experiments/` for saved metrics and artifacts\n",
    "\n",
    "### Key Files to Modify\n",
    "\n",
    "- `config/config.yaml` - Main configuration file\n",
    "- `docs/project_requirements.md` - Project documentation template\n",
    "- `src/features/build_features.py` - Add custom features\n",
    "- `src/models/train_model.py` - Add new algorithms\n",
    "- `src/evaluation/evaluate_model.py` - Add custom metrics\n",
    "\n",
    "### Resources\n",
    "\n",
    "- See `README.md` for complete documentation\n",
    "- See `QUICKSTART.md` for quick reference\n",
    "- Check inline code comments for detailed explanations\n",
    "- Review `docs/project_requirements.md` for project planning\n",
    "\n",
    "---\n",
    "\n",
    "**Good luck with your ML project! üöÄ**\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
